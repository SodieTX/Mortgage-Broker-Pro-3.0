# ============================================================
# INTEGRATION CONFIGURATION
# Defines how all 4 enhanced schemas work together
# ============================================================

name: "Mortgage Broker Pro 2.0 - Integrated Schema System"
version: "1.0.0"
description: "Complete integration configuration for all enhanced schemas"

# ============================================================
# LOAD ORDER
# ============================================================
load_order:
  - file: "00-Master-Integration-Schema.sql"
    description: "Master integration setup - MUST run first"
    critical: true
    
  - file: "EMC2-Complete-Schema-v2.0-Enhanced.sql"
    description: "Core domain model and workflow engine"
    dependencies: ["00-Master-Integration-Schema.sql"]
    
  - file: "Universal-Import-Logic-v6.0-Enhanced.sql"
    description: "Advanced import and data discovery system"
    dependencies: ["00-Master-Integration-Schema.sql", "EMC2-Complete-Schema-v2.0-Enhanced.sql"]
    
  - file: "Hermes-2.0-Enhanced.sql"
    description: "Enterprise import orchestration"
    dependencies: ["00-Master-Integration-Schema.sql", "Universal-Import-Logic-v6.0-Enhanced.sql"]
    
  - file: "Athena-7.0-Enhanced.sql"
    description: "ML-powered lender matching engine"
    dependencies: ["00-Master-Integration-Schema.sql", "EMC2-Complete-Schema-v2.0-Enhanced.sql"]

# ============================================================
# DATA FLOW PATHS
# ============================================================
data_flows:
  scenario_import_flow:
    name: "Scenario Data Import"
    description: "Complete flow from data import to lender matching"
    steps:
      1:
        component: "Universal Import"
        action: "Receive and validate raw data"
        outputs: ["import_id", "data_hash", "quality_score"]
        
      2:
        component: "Hermes"
        action: "Process and transform data"
        inputs: ["import_id"]
        outputs: ["job_id", "transformed_data"]
        
      3:
        component: "E=mcÂ²"
        action: "Create/update scenario"
        inputs: ["transformed_data"]
        outputs: ["scenario_id", "workflow_state"]
        
      4:
        component: "Athena"
        action: "Match lenders and evaluate"
        inputs: ["scenario_id"]
        outputs: ["matches", "recommendations"]

  real_time_stream_flow:
    name: "Real-Time Data Stream"
    description: "Streaming data processing pipeline"
    steps:
      1:
        component: "Universal Import"
        action: "Stream ingestion via Kafka/Kinesis"
        outputs: ["stream_id", "partition_key"]
        
      2:
        component: "Stream Processing"
        action: "Window aggregation and enrichment"
        outputs: ["enriched_data"]
        
      3:
        component: "ML Models"
        action: "Real-time scoring and prediction"
        outputs: ["predictions", "confidence"]

# ============================================================
# SHARED RESOURCES
# ============================================================
shared_resources:
  error_logging:
    table: "core.ErrorLog"
    function: "core.fn_log_error_unified"
    used_by: ["all"]
    
  tenant_context:
    function: "core.fn_get_current_tenant"
    used_by: ["all"]
    
  user_context:
    function: "core.fn_get_current_user"
    used_by: ["all"]
    
  quality_scores:
    table: "universal_import.QualityScores"
    used_by: ["Universal Import", "Hermes", "Athena"]
    
  ml_models:
    schema: "ml_models"
    used_by: ["Universal Import", "Athena"]

# ============================================================
# INTEGRATION POINTS
# ============================================================
integration_points:
  universal_to_hermes:
    bridge_table: "import.UniversalHermesBridge"
    sync_function: "import.fn_sync_universal_import"
    conflict_resolution: "Universal Import takes precedence"
    
  emc2_to_athena:
    bridge_table: "core.EvaluationModels"
    event_trigger: "workflow.scenario_status_change"
    activation_condition: "Status = 'MATCHING'"
    
  import_to_workflow:
    bridge_table: "workflow.ScenarioImports"
    orchestration_function: "core.fn_orchestrate_scenario_import"
    
  ml_to_all:
    model_registry: "ml_models.ModelRegistry"
    prediction_cache: "ml_models.PredictionCache"
    feature_store: "ml_models.FeatureStore"

# ============================================================
# CONFLICT RESOLUTION
# ============================================================
conflict_resolution:
  duplicate_imports:
    detection: "SHA-256 hash comparison"
    resolution: "Skip duplicate, return existing import_id"
    
  schema_evolution:
    detection: "Version mismatch in attribute_discovery"
    resolution: "Auto-migrate using ML-suggested mappings"
    
  concurrent_updates:
    detection: "Optimistic locking with version columns"
    resolution: "Retry with exponential backoff"
    
  data_quality_threshold:
    detection: "Quality score below configured minimum"
    resolution: "Route to manual review queue"

# ============================================================
# PERFORMANCE OPTIMIZATION
# ============================================================
performance:
  caching:
    redis_enabled: true
    cache_layers:
      - name: "Athena Results Cache"
        ttl: 300
        key_pattern: "athena:eval:{scenario_id}"
        
      - name: "ML Predictions Cache"
        ttl: 3600
        key_pattern: "ml:predict:{model_id}:{input_hash}"
        
      - name: "Import Validation Cache"
        ttl: 86400
        key_pattern: "import:valid:{data_hash}"
  
  indexing:
    critical_indexes:
      - table: "workflow.Scenarios"
        columns: ["Scenario_ID", "Status", "Created_At"]
        type: "btree"
        
      - table: "universal_import.ImportRaw"
        columns: ["import_id", "data_hash"]
        type: "hash"
        
      - table: "ml_models.Embeddings"
        columns: ["embedding"]
        type: "ivfflat"
        
  partitioning:
    tables:
      - name: "core.ErrorLog"
        strategy: "range"
        column: "created_at"
        interval: "monthly"
        
      - name: "import.ImportData"
        strategy: "list"
        column: "tenant_id"

# ============================================================
# MONITORING & ALERTS
# ============================================================
monitoring:
  health_check_function: "core.fn_system_health_check"
  check_interval: 60  # seconds
  
  alerts:
    - name: "Import Quality Degradation"
      condition: "AVG(quality_score) < 0.6 over 1 hour"
      severity: "WARNING"
      
    - name: "Athena Latency High"
      condition: "P95 latency > 5000ms"
      severity: "CRITICAL"
      
    - name: "Error Rate Spike"
      condition: "Error count > 100 in 5 minutes"
      severity: "CRITICAL"
  
  dashboards:
    - name: "System Overview"
      metrics:
        - "Import volume by source"
        - "Scenario creation rate"
        - "Lender match success rate"
        - "ML model performance"
        
    - name: "Data Quality"
      metrics:
        - "Quality scores by import type"
        - "Validation failure reasons"
        - "PII detection rate"
        - "Schema evolution frequency"

# ============================================================
# SECURITY & COMPLIANCE
# ============================================================
security:
  encryption:
    at_rest: "AES-256"
    in_transit: "TLS 1.3"
    pii_fields: "Double encryption with separate keys"
    
  access_control:
    authentication: "OAuth 2.0 / SAML"
    authorization: "Row Level Security + RBAC"
    audit_trail: "Blockchain-backed immutable log"
    
  compliance:
    frameworks: ["SOC2", "GDPR", "CCPA", "GLBA"]
    pii_handling: "Automatic detection and masking"
    data_retention: "Configurable by tenant"
    right_to_forget: "Soft delete with crypto-shredding"

# ============================================================
# DEPLOYMENT CONFIGURATION
# ============================================================
deployment:
  environments:
    development:
      auto_migrate: true
      sample_data: true
      ml_models: "mock"
      
    staging:
      auto_migrate: false
      sample_data: false
      ml_models: "sandbox"
      
    production:
      auto_migrate: false
      sample_data: false
      ml_models: "production"
      backup_before_deploy: true
  
  rollback_strategy:
    method: "Blue-Green deployment"
    verification_time: 300  # seconds
    automatic_rollback: true
    
# ============================================================
# EXTENSION REQUIREMENTS
# ============================================================
extensions:
  required:
    - name: "uuid-ossp"
      version: "1.1"
    - name: "btree_gist"
      version: "1.5"
    - name: "pg_trgm"
      version: "1.5"
    - name: "postgis"
      version: "3.0+"
    - name: "pgcrypto"
      version: "1.3"
    
  optional:
    - name: "timescaledb"
      version: "2.0+"
      fallback: "Standard PostgreSQL tables"
    - name: "vector"
      version: "0.4+"
      fallback: "Array-based embeddings"
    - name: "pg_cron"
      version: "1.4+"
      fallback: "External scheduler"

# ============================================================
# TESTING CONFIGURATION
# ============================================================
testing:
  test_data_generator: "core.fn_generate_test_data"
  
  test_suites:
    - name: "Integration Tests"
      scope: "Cross-schema data flow"
      runner: "pytest"
      
    - name: "Performance Tests"
      scope: "Load and latency testing"
      runner: "k6"
      
    - name: "Security Tests"
      scope: "Penetration and vulnerability"
      runner: "OWASP ZAP"
  
  continuous_testing:
    enabled: true
    on_commit: ["unit", "integration"]
    nightly: ["performance", "security"]
    
# ============================================================
# DOCUMENTATION
# ============================================================
documentation:
  auto_generate: true
  formats: ["Markdown", "HTML", "PDF"]
  
  sections:
    - "System Architecture"
    - "Data Flow Diagrams"
    - "API Reference"
    - "Deployment Guide"
    - "Troubleshooting"
    
  tools:
    erd_generator: "dbdocs.io"
    api_docs: "OpenAPI 3.0"
    flow_diagrams: "Mermaid"
